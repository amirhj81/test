# summary_lmstudio_fullpage.py
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from duckduckgo_search import DDGS
import requests
from bs4 import BeautifulSoup
import re

# =====================
# ØªØ¹Ø±ÛŒÙ State
# =====================
class State(TypedDict):
    messages: list
    need_search: str

# =====================
# Ø§ØªØµØ§Ù„ Ø¨Ù‡ LM Studio
# =====================
llm = ChatOpenAI(
    model="gemma-3-4b-it@q4_k_m",
    openai_api_base="http://localhost:4050/v1",
    openai_api_key="lmstudio",
    temperature=0.1
)

# ---------- ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ ----------
def fetch_text(url: str, max_chars: int = 3000) -> str:
    """
    ÛŒÚ© URL Ø±Ø§ Ú¯Ø±ÙØªÙ‡ Ùˆ Ù…ØªÙ† Ù‚Ø§Ø¨Ù„â€ŒØ®ÙˆØ§Ù†Ø¯Ù† Ø¢Ù† Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    Ø§Ú¯Ø± Ø®Ø·Ø§ Ø±Ø® Ø¯Ù‡Ø¯ØŒ Ù¾ÛŒØ§Ù… Ø®Ø·Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.
    """
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, timeout=8)
        r.raise_for_status()
    except Exception as e:
        return f"[Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ {url}: {e}]"

    soup = BeautifulSoup(r.text, "lxml")
    for tag in soup(["script", "style", "nav", "footer", "aside"]):
        tag.extract()
    text = soup.get_text(separator=" ", strip=True)
    text = re.sub(r"\s+", " ", text)
    return text[:max_chars]

# ---------- Nodes ----------
def check_need_search(state: State):
    user_msg = state["messages"][-1].content
    response = llm.invoke([
        HumanMessage(content=f"Ø¢ÛŒØ§ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø²ÛŒØ± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø§ÛŒÙ†ØªØ±Ù†Øª Ù‡Ø³ØªØŸ ÙÙ‚Ø· Ø¬ÙˆØ§Ø¨ 'Ø¨Ù„Ù‡' ÛŒØ§ 'Ø®ÛŒØ±'.\n\nÙ…ÙˆØ¶ÙˆØ¹: {user_msg}")
    ])
    return {"need_search": "Ø¨Ù„Ù‡" if "Ø¨Ù„Ù‡" in response.content else "Ø®ÛŒØ±"}

def search(state: State):
    user_msg = state["messages"][-1].content

    # Ø¹Ø¨Ø§Ø±Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©ÙˆØªØ§Ù‡
    optimized_query_msg = HumanMessage(
        content=f"Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ ØªÙ†Ù‡Ø§ Ú†Ù†Ø¯ Ú©Ù„Ù…Ù‡Ù” Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø§Ø­ØªÙ…Ø§Ù„ Ø¯Ø§Ø±Ø¯ Ø¯Ø± Ø§ÛŒÙ†ØªØ±Ù†Øª Ù†ØªÛŒØ¬Ù‡ Ø¨Ø¯Ù‡Ø¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ØŒ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø§Ø¶Ø§ÙÙ‡:\n{user_msg}"
    )
    optimized_query = llm.invoke([optimized_query_msg]).content.strip().splitlines()[0]
    print(f"ğŸ” Ø¹Ø¨Ø§Ø±Øª Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡: {optimized_query}")

    # Ú¯Ø±ÙØªÙ† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² DuckDuckGo
    links = []
    try:
        for item in DDGS().text(optimized_query, max_results=3):
            links.append(item["href"])
    except Exception as e:
        links = []

    # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù‡Ø± Ù„ÛŒÙ†Ú©
    full_texts = []
    for url in links:
        print(f"â¬‡ï¸  Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯: {url}")
        page_text = fetch_text(url)
        full_texts.append(f"Ù…Ù†Ø¨Ø¹: {url}\n{page_text}\n")

    combined_text = "\n".join(full_texts).strip()
    print("ğŸ“„ Ù…ØªÙ† Ú©Ø§Ù…Ù„ ØµÙØ­Ø§Øª:\n" + (combined_text or "Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.") + "\n")

    if not combined_text:
        combined_text = f"Ù‡ÛŒÚ† Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª Ø¨Ø±Ø§ÛŒ '{user_msg}' Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯."

    final_msg = HumanMessage(
        content=f"Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ±ØŒ ÛŒÚ© Ù…ØªÙ† ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ùˆ Ù…Ù†Ø³Ø¬Ù… Ø¨Ù†ÙˆÛŒØ³ (Ú©Ù…ØªØ±ÛŒÙ† Ø­Ø§Ù„Øª Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø­Ø¯Ø§Ú©Ø«Ø± Û³-Ûµ Ø®Ø·):\n\n{combined_text}"
    )
    final_response = llm.invoke([final_msg])
    return {"messages": state["messages"] + [HumanMessage(content=combined_text)]
    }

def filter_relevant_paragraphs(state: State):
    """
    Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØªÛŒ Ø§Ø² ÙˆØ¨ Ø±Ø§ Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ø¯Ù‡
    Ùˆ ÙÙ‚Ø· Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ø§ØµÙ„ÛŒ Ø±Ø§ Ø­ÙØ¸ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    user_msg = state["messages"][0].content          # Ù¾ÛŒØ§Ù… Ø§ÙˆÙ„ÛŒÙ‡ Ú©Ø§Ø±Ø¨Ø±
    raw_text = state["messages"][-1].content
    combined_text = state["messages"][-1].content    # Ù…ØªÙ† Ø®Ø§Ù… Ø¯Ø§Ù†Ù„ÙˆØ¯â€ŒØ´Ø¯Ù‡

    paragraphs = [p.strip() for p in combined_text.split("\n") if p.strip()]

    # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø¯Ø± ØªÙˆÚ©Ù†
    batch_size = 10
    relevant_chunks = []

    for i in range(0, len(paragraphs), batch_size):
        batch = "\n".join(paragraphs[i:i+batch_size])
        prompt = (
            f"Ù…ÙˆØ¶ÙˆØ¹ Ø§ØµÙ„ÛŒ: {user_msg}\n\n"
            f"Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†. ÙÙ‚Ø· Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø§ØµÙ„ÛŒ Ù…Ø±ØªØ¨Ø·â€ŒØ§Ù†Ø¯ "
            f"Ø±Ø§ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø› Ø¨Ù‚ÛŒÙ‡ Ø±Ø§ Ø­Ø°Ù Ú©Ù†. "
            f"Ø§Ú¯Ø± Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù… Ù…Ø±ØªØ¨Ø· Ù†Ø¨ÙˆØ¯Ù†Ø¯ØŒ Ø¨Ù†ÙˆÛŒØ³: Â«Ù‡ÛŒÚ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯Â».\n\n{batch}"
        )
        response = llm.invoke([HumanMessage(content=prompt)]).content.strip()
        if "Ù‡ÛŒÚ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯" not in response:
            relevant_chunks.append(response)

    filtered_text = "\n".join(relevant_chunks) or "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
    return {"messages": state["messages"][:-1] + [HumanMessage(content=filtered_text)]}

def llm_only(state: State):
    user_msg = state["messages"][-1].content
    response = llm.invoke([HumanMessage(content=f"Ù…ÙˆØ¶ÙˆØ¹ Ø²ÛŒØ± Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡:\n\n{user_msg}")])
    return {"messages": state["messages"] + [response]}

def summarize(state: State):
    all_msgs = "\n".join([m.content for m in state["messages"][1:]])
    response = llm.invoke([HumanMessage(content=f"Ù…Ø·Ø§Ù„Ø¨ Ø²ÛŒØ± Ø±Ø§ Ø®Ù„Ø§ØµÙ‡ Ú©Ù† (Ú©Ù…ØªØ±ÛŒÙ† Ø­Ø§Ù„Øª Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø­Ø¯Ø§Ú©Ø«Ø± Û³-Ûµ Ø®Ø·):\n\n{all_msgs}")])
    return {"messages": state["messages"] + [response]}

# =====================
# Ø³Ø§Ø®Øª Ú¯Ø±Ø§Ù
# =====================
graph_builder = StateGraph(State)
graph_builder.add_node("check_need_search", check_need_search)
graph_builder.add_node("search", search)
graph_builder.add_node("llm_only", llm_only)
graph_builder.add_node("summarize", summarize)
graph_builder.add_node("filter_relevant_paragraphs", filter_relevant_paragraphs)

graph_builder.set_entry_point("check_need_search")
graph_builder.add_conditional_edges(
    "check_need_search",
    lambda state: "search" if state["need_search"] == "Ø¨Ù„Ù‡" else "llm_only",
    {"search": "search", "llm_only": "llm_only"}
)
graph_builder.add_edge("search", "filter_relevant_paragraphs")
graph_builder.add_edge("filter_relevant_paragraphs", "summarize")
graph_builder.add_edge("llm_only", END)
graph_builder.add_edge("summarize", END)

graph = graph_builder.compile()

# =====================
# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
# =====================
if __name__ == "__main__":
    topic = input("Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ")
    result = graph.invoke({"messages": [HumanMessage(content=topic)], "need_search": ""})
    print("\nğŸ“Œ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:\n")
    for m in result["messages"]:
        print(m.content)